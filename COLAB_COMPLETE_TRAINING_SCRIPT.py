"""
üöÄ COMPLETE ARAI SALIENCY TRAINING SCRIPT FOR GOOGLE COLAB
===========================================================

Copy this entire file into a single Colab cell and run it!
This will:
1. Check GPU availability
2. Mount Google Drive
3. Clone your repository
4. Generate 5000 synthetic UI training samples
5. Train the model (~1-2 hours)
6. Save trained model to Google Drive

Total time: ~1.5-2 hours
Result: Production-quality saliency model for UI analysis
"""

# ========================================
# STEP 1: CHECK GPU
# ========================================
import torch
print("=" * 60)
print("üîß CHECKING GPU AVAILABILITY")
print("=" * 60)
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"‚úÖ GPU: {torch.cuda.get_device_name(0)}")
    print(f"‚úÖ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
else:
    print("‚ö†Ô∏è  WARNING: No GPU detected!")
    print("   Go to: Runtime ‚Üí Change runtime type ‚Üí GPU")
print()

# ========================================
# STEP 2: MOUNT GOOGLE DRIVE
# ========================================
print("=" * 60)
print("üìÅ MOUNTING GOOGLE DRIVE")
print("=" * 60)
from google.colab import drive
drive.mount('/content/drive')

import os
os.makedirs('/content/drive/MyDrive/ARAI/models', exist_ok=True)
os.makedirs('/content/drive/MyDrive/ARAI/training_logs', exist_ok=True)
print("‚úÖ Google Drive mounted and directories created!")
print()

# ========================================
# STEP 3: CLONE REPOSITORY
# ========================================
print("=" * 60)
print("üì• CLONING ARAI REPOSITORY")
print("=" * 60)
import os
if os.path.exists('/content/arai'):
    print("Repository already exists, updating...")
    !cd /content/arai && git pull
else:
    !git clone https://github.com/kavishaniy/ARAI-System.git /content/arai

os.chdir('/content/arai/backend')
print("‚úÖ Repository ready!")
print()

# ========================================
# STEP 4: INSTALL DEPENDENCIES
# ========================================
print("=" * 60)
print("üì¶ INSTALLING DEPENDENCIES")
print("=" * 60)
!pip install -q pillow scipy tqdm matplotlib numpy opencv-python
print("‚úÖ Dependencies installed!")
print()

# ========================================
# STEP 5: GENERATE SYNTHETIC UI DATASET
# ========================================
print("=" * 60)
print("üé® GENERATING SYNTHETIC UI TRAINING DATA")
print("=" * 60)
print("Creating 5000 realistic UI designs with saliency maps...")
print("This will take ~10-15 minutes\n")

import numpy as np
import cv2
from PIL import Image
from tqdm import tqdm
import random

# Create directories
os.makedirs('/content/ui_training_data/images', exist_ok=True)
os.makedirs('/content/ui_training_data/maps', exist_ok=True)

def create_ui_sample(idx):
    """Generate realistic UI design with corresponding saliency map"""
    
    # Create white canvas
    img = np.ones((256, 256, 3), dtype=np.uint8) * 245
    saliency = np.zeros((256, 256), dtype=np.float32)
    
    # Modern color palette
    colors = [
        (66, 133, 244),   # Google Blue
        (52, 168, 83),    # Google Green
        (234, 67, 53),    # Google Red
        (251, 188, 5),    # Google Yellow
        (156, 39, 176),   # Material Purple
        (0, 150, 136),    # Teal
        (255, 87, 34),    # Deep Orange
    ]
    
    # Add header bar (medium saliency)
    header_color = random.choice(colors)
    header_height = random.randint(40, 55)
    cv2.rectangle(img, (0, 0), (256, header_height), header_color, -1)
    
    # Header text
    texts = ['Dashboard', 'App Title', 'Navigation', 'MyApp', 'Portal']
    cv2.putText(img, random.choice(texts), (60, header_height-15), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)
    saliency[0:header_height, :] = 0.5
    
    # Add primary CTA button (highest saliency)
    btn_y = random.randint(80, 140)
    btn_width = random.randint(150, 190)
    btn_x_start = (256 - btn_width) // 2
    btn_color = random.choice([c for c in colors if c != header_color])
    
    cv2.rectangle(img, (btn_x_start, btn_y), (btn_x_start + btn_width, btn_y+40), btn_color, -1)
    
    btn_texts = ['Get Started', 'Sign Up', 'Learn More', 'Take Action', 'Continue', 'Submit']
    cv2.putText(img, random.choice(btn_texts), (btn_x_start + 20, btn_y+25), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2)
    saliency[btn_y:btn_y+40, btn_x_start:btn_x_start+btn_width] = 1.0
    
    # Add form fields or cards (medium saliency)
    num_elements = random.randint(1, 3)
    for i in range(num_elements):
        elem_y = 65 + header_height + i * 45
        if elem_y + 35 < 220:  # Don't overlap with bottom
            cv2.rectangle(img, (30, elem_y), (226, elem_y+30), (220, 220, 220), 2)
            cv2.putText(img, 'Input field', (40, elem_y+20), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.35, (150,150,150), 1)
            saliency[elem_y:elem_y+30, 30:226] = 0.4
    
    # Add text content (low-medium saliency)
    text_start_y = 180
    content_texts = [
        'Lorem ipsum dolor',
        'Feature description',
        'Additional information',
        'Learn more about this',
    ]
    for i in range(min(3, len(content_texts))):
        text_y = text_start_y + i * 18
        if text_y < 240:
            cv2.putText(img, content_texts[i], (35, text_y), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.35, (100,100,100), 1)
            saliency[text_y-10:text_y+5, 35:200] = 0.25
    
    # Add icon/logo area (medium-high saliency)
    if random.random() > 0.6:
        icon_x, icon_y = random.randint(15, 30), header_height + 20
        cv2.circle(img, (icon_x + 15, icon_y + 15), 15, random.choice(colors), -1)
        saliency[icon_y:icon_y+30, icon_x:icon_x+30] = 0.7
    
    # Add footer (low saliency)
    if random.random() > 0.5:
        footer_y = 235
        cv2.rectangle(img, (0, footer_y), (256, 256), (230, 230, 230), -1)
        cv2.putText(img, 'Footer', (100, footer_y+13), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.3, (150,150,150), 1)
        saliency[footer_y:256, :] = 0.15
    
    # Add center bias (humans naturally look at center)
    y, x = np.ogrid[:256, :256]
    center_bias = np.exp(-((x-128)**2 + (y-128)**2) / (2*90**2))
    saliency = 0.70 * saliency + 0.30 * center_bias
    
    # Smooth transitions with Gaussian blur
    saliency = cv2.GaussianBlur(saliency, (21, 21), 8)
    
    # Normalize to 0-255
    saliency = (saliency * 255).astype(np.uint8)
    
    return img, saliency

# Generate 5000 samples
print("Generating samples (this takes ~10 minutes)...")
for i in tqdm(range(5000), desc="Creating UI samples"):
    img, smap = create_ui_sample(i)
    
    # Save files
    Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)).save(
        f'/content/ui_training_data/images/ui_{i:05d}.jpg', quality=95
    )
    Image.fromarray(smap).save(
        f'/content/ui_training_data/maps/ui_{i:05d}.png'
    )

# Verify
image_count = len(os.listdir('/content/ui_training_data/images'))
map_count = len(os.listdir('/content/ui_training_data/maps'))
print(f"\n‚úÖ Generated {image_count} images and {map_count} saliency maps!")
print(f"üìÅ Location: /content/ui_training_data/")

# Show sample
import matplotlib.pyplot as plt
print("\nüì∏ Sample Preview:")
sample_img = Image.open('/content/ui_training_data/images/ui_00000.jpg')
sample_map = Image.open('/content/ui_training_data/maps/ui_00000.png')

fig, axes = plt.subplots(1, 2, figsize=(12, 5))
axes[0].imshow(sample_img)
axes[0].set_title('Sample UI Image', fontsize=14, fontweight='bold')
axes[0].axis('off')
axes[1].imshow(sample_map, cmap='hot')
axes[1].set_title('Ground Truth Saliency Map', fontsize=14, fontweight='bold')
axes[1].axis('off')
plt.tight_layout()
plt.show()
print()

# ========================================
# STEP 6: TRAIN THE MODEL
# ========================================
print("=" * 60)
print("üöÄ STARTING MODEL TRAINING")
print("=" * 60)
print("Training Configuration:")
print("  ‚Ä¢ Dataset: 5000 synthetic UI samples")
print("  ‚Ä¢ Batch size: 16")
print("  ‚Ä¢ Epochs: 40")
print("  ‚Ä¢ Learning rate: 1e-4")
print("  ‚Ä¢ Device: GPU (CUDA)")
print("  ‚Ä¢ Estimated time: 1-2 hours")
print("\n‚è≥ Training in progress...\n")

os.chdir('/content/arai/backend/training')

# Run training
!python train_saliency.py \
    --image_dir /content/ui_training_data/images \
    --saliency_dir /content/ui_training_data/maps \
    --batch_size 16 \
    --num_epochs 40 \
    --learning_rate 1e-4 \
    --save_dir /content/drive/MyDrive/ARAI/models \
    --device cuda

# ========================================
# STEP 7: COMPLETION & NEXT STEPS
# ========================================
print("\n" + "=" * 60)
print("‚úÖ TRAINING COMPLETE!")
print("=" * 60)
print("\nüìÅ Model saved to:")
print("   /content/drive/MyDrive/ARAI/models/saliency_model.pth")
print("\nüì• To download the model:")
print("   1. Option A: Download from Google Drive directly")
print("      ‚Üí Open Drive ‚Üí MyDrive ‚Üí ARAI ‚Üí models ‚Üí Download")
print("   2. Option B: Run this cell:")
print("      from google.colab import files")
print("      files.download('/content/drive/MyDrive/ARAI/models/saliency_model.pth')")
print("\nüîó To integrate with your app:")
print("   1. Place model in: backend/models/saliency_model.pth")
print("   2. Restart your backend server")
print("   3. The model will be auto-detected!")
print("\nüéâ You now have a production-quality saliency model!")
print("=" * 60)
